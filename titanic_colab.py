# -*- coding: utf-8 -*-
"""Titanic_colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PPBDFfG65Ii8tuH2lwxw0yS-ku_r4UEk
"""

# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE
# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.
import kagglehub
kagglehub.login()

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

titanic_path = kagglehub.competition_download('titanic')

print('Data source import complete.')

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

df = pd.read_csv("train.csv")
df_test = pd.read_csv('test.csv')

# Extract Title from Name for both train and test sets
for dataset in [df, df_test]:
    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)
    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Other')
    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')
    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')

df["Age"].fillna(df["Age"].median(), inplace=True)

df = df.dropna(subset=["Embarked"])
df.isnull().sum()

df.reset_index(drop=True, inplace=True)

# Add 'Title' to features
X = df[["Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked", "Title"]]
y = df["Survived"]

X = pd.get_dummies(X, columns=["Sex", "Embarked", "Title"], drop_first=True)
X = pd.get_dummies(X, columns=["Pclass"])

X = X.astype(int)
X

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = .2 , random_state = 43)

import tensorflow as tf
from tensorflow import keras

model = tf.keras.Sequential([
    tf.keras.layers.Dense(16, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(1, activation = 'sigmoid')
])
model.compile(optimizer = 'adam', loss='binary_crossentropy', metrics=['accuracy'] )
model.fit(
    X_train,
    y_train,
    epochs=50,         # number of times to loop over the data
    batch_size=32,     # how many samples per gradient update
    validation_split=0.2,  # use 20% of training data for validation
    verbose=1          # show training progress
)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
y_pred_prob = model.predict(X_test)
y_pred = (y_pred_prob > 0.5).astype(int)

y_pred.shape

# 2. Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# 3. Print them nicely
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

# 4. Optional: full classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

model.fit(
    X_train_resampled, y_train_resampled,
    epochs=20,
    batch_size=32,
    validation_split=0.2,
    verbose=1
)

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

# Predict probabilities
y_pred_prob = model.predict(X_test)

# Convert to binary labels (threshold = 0.5)
y_pred = (y_pred_prob > 0.5).astype(int)

# Metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

# Print metrics
print(f"Accuracy:  {accuracy:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Recall:    {recall:.4f}")
print(f"F1 Score:  {f1:.4f}")

# Optional: classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

df_test["Age"].fillna(df_test["Age"].median(), inplace=True)
df_test = df_test.dropna(subset=["Embarked"])
df_test.reset_index(drop=True, inplace=True)

X_f = df_test[["Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Embarked", "Title"]]
X_f = pd.get_dummies(X_f, columns=["Sex", "Embarked", "Title"], drop_first=True)
X_f = pd.get_dummies(X_f, columns=["Pclass"])
X_f['Fare'].fillna(X_f['Fare'].median(), inplace=True)
X_f = X_f.astype(int)

# Align test set columns to train set columns
X_f = X_f.reindex(columns = X.columns, fill_value=0)

y_final= model.predict(X_f)
y_final_prediction = (y_final > 0.5).astype(int)

y_final_prediction[:5]

submission_df = pd.DataFrame({
    'PassengerId': df_test['PassengerId'],
    'Survived': y_final_prediction.ravel()
})
submission_df

submission_df.to_csv('titanic_submission.csv', index=False)